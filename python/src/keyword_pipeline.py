import json
import os
from src.hadoop.hdfs_client import HDFSClient
from src.keyword.analysis import KeywordAnalyzer
from dotenv import load_dotenv
from db.database import PostgresImporter
import re

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œ ì‚¬ì „ (í•„ìš”ì— ë”°ë¼ í™•ì¥ ê°€ëŠ¥)
FOOD_KEYWORDS = {
  # ê¸°ë³¸ ìŒì‹ ì¹´í…Œê³ ë¦¬
  'ìŒì‹', 'ìš”ë¦¬', 'ë§›', 'ì‹ê°', 'í’ë¯¸', 'í–¥', 'ì‹ì‚¬', 'ë©”ë‰´', 'ìš”ë¦¬ë²•',

  # ë§› ê´€ë ¨ í‘œí˜„
  'ë§¤ì½¤', 'ë‹¬ì½¤', 'ì§­ì§¤', 'ê³ ì†Œ', 'ì‹ ì„ ', 'ë¶€ë“œëŸ¬ìš´', 'ì«„ê¹ƒ', 'ë‹´ë°±', 'ì§œë‹¤', 'ë‹¬ë‹¤', 'ë§µë‹¤', 'ì‹ ë§›',
  'ì“´ë§›',
  'ê°ì¹ ë§›', 'ê°ì¹ ë§›ë‚˜ëŠ”', 'ë§›ìˆ', 'ë§›ìˆëŠ”', 'ë§›ìˆì—ˆ', 'ë§›ì—†', 'ë§›ì´ ì¢‹', 'ë§›ì´ í›Œë¥­',

  # í•œì‹
  'ë°¥', 'ê¹€ì¹˜', 'ëœì¥', 'ê³ ì¶”ì¥', 'ë–¡', 'êµ­ìˆ˜', 'ë¹„ë¹”ë°¥', 'ë¶ˆê³ ê¸°', 'ê°ˆë¹„', 'ì°Œê°œ', 'íƒ•', 'ì „', 'ë‚˜ë¬¼',
  'ë³´ìŒˆ', 'ì¡±ë°œ', 'ì‚¼ê²¹ì‚´', 'ì œìœ¡', 'ê¹€ë°¥', 'ìˆœë‘ë¶€', 'ê³±ì°½', 'ìœ¡íšŒ', 'ë‹­ê°ˆë¹„', 'ìŒˆ', 'êµ­',

  # ì¤‘ì‹
  'ì§œì¥ë©´', 'ì§¬ë½•', 'ë³¶ìŒë°¥', 'íƒ•ìˆ˜ìœ¡', 'ë§Œë‘', 'ë§ˆë¼', 'ë§ˆë¼íƒ•', 'ì–‘ì¥í”¼', 'ê¹í’ê¸°', 'í› ê¶ˆ', 'ë”¤ì„¬',

  # ì¼ì‹
  'ì´ˆë°¥', 'ìŠ¤ì‹œ', 'ì‚¬ì‹œë¯¸', 'ë¼ë©˜', 'ìš°ë™', 'ì†Œë°”', 'ëˆë¶€ë¦¬', 'ë®ë°¥', 'ê°€ì¸ ë™', 'ì˜¤ë‹ˆê¸°ë¦¬', 'íƒ€ì½”ì•¼í‚¤',
  'ì•¼í‚¤ë‹ˆì¿ ', 'ìƒ¤ë¸Œìƒ¤ë¸Œ', 'ì¹´ë ˆ', 'ê°€ë¼ì•„ê²Œ', 'ê·œë™', 'í…ë™', 'ì†Œí…Œë¦¬ì•„í‚¤',

  # ì–‘ì‹
  'íŒŒìŠ¤íƒ€', 'í”¼ì', 'ìŠ¤í…Œì´í¬', 'í–„ë²„ê±°', 'ìƒŒë“œìœ„ì¹˜', 'ë¦¬ì¡°ë˜', 'ìƒëŸ¬ë“œ', 'ë¸Œë£¨ìŠ¤ì¼€íƒ€', 'ì˜¤ë¯ˆë ›', 'ë¹ ë„¤',

  # ë””ì €íŠ¸/ìŒë£Œ
  'ì¼€ì´í¬', 'ë¹µ', 'ì¿ í‚¤', 'ë§ˆì¹´ë¡±', 'ì•„ì´ìŠ¤í¬ë¦¼', 'ì™€í”Œ', 'í¬ë¡œí”Œ', 'íƒ€ë¥´íŠ¸', 'ë¬´ìŠ¤', 'í‘¸ë”©', 'ì»¤í”¼',
  'ë¼ë–¼', 'ì—ìŠ¤í”„ë ˆì†Œ', 'ì•„ë©”ë¦¬ì¹´ë…¸', 'ì£¼ìŠ¤', 'ì°¨', 'ë§¥ì£¼', 'ì™€ì¸', 'ì¹µí…Œì¼', 'ì†Œì£¼', 'ë§‰ê±¸ë¦¬',

  # ì‹ì¬ë£Œ
  'ì†Œê³ ê¸°', 'ë¼ì§€ê³ ê¸°', 'ë‹­ê³ ê¸°', 'ìƒì„ ', 'í•´ì‚°ë¬¼', 'ì•¼ì±„', 'ì±„ì†Œ', 'ê³¼ì¼', 'ìŒ€', 'ë°€ê°€ë£¨', 'ì½©', 'ì¹˜ì¦ˆ',
  'ë²„ì„¯', 'ë‹¬ê±€', 'ê³„ë€', 'ê°ì', 'ê³ êµ¬ë§ˆ', 'í† ë§ˆí† ', 'ì–‘íŒŒ', 'ë§ˆëŠ˜', 'ìƒê°•', 'íŒŒ', 'ê¹€', 'íŒŒë˜',

  # ì¡°ë¦¬ ë°©ì‹
  'êµ¬ì´', 'ë³¶ìŒ', 'ì°œ', 'ì¡°ë¦¼', 'íŠ€ê¹€', 'ì‚¶ê¸°', 'ë°ì¹˜ê¸°', 'ë¬´ì¹¨', 'íšŒ', 'í›ˆì œ', 'ë¡œìŠ¤íŒ…', 'ë² ì´í‚¹',

  # , 'ì‹ì‚¬ í˜•íƒœ
  'ì½”ìŠ¤', 'ë·”í˜', 'í•œìƒ', 'ì •ì‹', 'ë°˜ìƒ', 'ë°±ë°˜', 'ë„ì‹œë½', 'í¬ì¥',

  # ê¸°íƒ€ ìŒì‹ ê´€ë ¨
  'ì–‘ë…', 'ì†ŒìŠ¤', 'ë“œë ˆì‹±', 'í† í•‘', 'ê°€ë‹ˆì‰¬', 'ë¹„ì£¼ì–¼', 'í”Œë ˆì´íŒ…', 'í¬ì…˜', 'ì‚¬ì´ë“œ', 'ë°˜ì°¬'
}

# ì œì™¸í•  í‚¤ì›Œë“œ ëª©ë¡ (ì›í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì—¬ê¸°ì— ì¶”ê°€)
EXCLUDED_KEYWORDS = {
  'ë§›ì§‘', 'ë©”ë‰´', 'í˜¼ë°¥', 'ì‹ì‚¬', 'ì£¼ì°¨', 'ë¡¯ë°', 'ë¦¬ì•„','ì†Œì£¼','ë§¥ì£¼','í¬ì¥'
}


def extract_sentence_with_keyword(text, keyword):
  """
  í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜

  Args:
      text (str): ì „ì²´ ë¦¬ë·° í…ìŠ¤íŠ¸
      keyword (str): ì°¾ì„ í‚¤ì›Œë“œ

  Returns:
      list: í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
  """
  # ë¬¸ì¥ êµ¬ë¶„ìë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬
  sentences = re.split(r'[.!?]\s*', text)

  # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë§Œ í•„í„°ë§
  keyword_sentences = []
  for sentence in sentences:
    if keyword in sentence:
      # ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆë›°ê¸° (ì˜ë¯¸ ì—†ëŠ” ë¬¸ì¥ ë°©ì§€)
      if len(sentence.strip()) < 5:
        continue

      # ë°©ë¬¸ ì •ë³´ ë° ë©”íƒ€ë°ì´í„° ì œê±°
      # 1. ë°©ë¬¸ ì‹œê°„ ë° ì˜ˆì•½ ì •ë³´ ì œê±°
      clean_sentence = re.sub(
          r'^(?:ì €ë…|ì ì‹¬|ì•„ì¹¨|ë°¤)ì— ë°©ë¬¸(?:ì˜ˆì•½ ì—†ì´ ì´ìš©)?(?:ëŒ€ê¸° ì‹œê°„ ë°”ë¡œ ì…ì¥)?(?:ì¼ìƒ|ì¹œëª©|ë°ì´íŠ¸|íšŒì‹|ê°€ì¡±ëª¨ì„)?',
          '', sentence)

      # 2. ë°©ë¬¸ì ì •ë³´ ì œê±°
      clean_sentence = re.sub(r'(?:ì§€ì¸ãƒ»ë™ë£Œ|ì¹œêµ¬|ì—°ì¸ãƒ»ë°°ìš°ì|ê°€ì¡±|í˜¼ì)', '',
                              clean_sentence)

      # 3. ë³„ì  ì •ë³´ ì œê±°
      clean_sentence = re.sub(r'^ë³„ì  \d+ ì ', '', clean_sentence)

      # 4. í‚¤ì›Œë“œ ë°˜ì‘ ì œê±° (ì˜ˆ: "ìŒì‹ì´ ë§›ìˆì–´ìš”+3")
      clean_sentence = re.sub(
          r'(?:ìŒì‹ì´ ë§›ìˆì–´ìš”|ê°€ì„±ë¹„ê°€ ì¢‹ì•„ìš”|ì¹œì ˆí•´ìš”|ì¬ë£Œê°€ ì‹ ì„ í•´ìš”|ë§¤ì¥ì´ ì²­ê²°í•´ìš”|ì–‘ì´ ë§ì•„ìš”|í˜¼ë°¥í•˜ê¸° ì¢‹ì•„ìš”|ë§¤ì¥ì´ ë„“ì–´ìš”)(?:\+\d+)?',
          '', clean_sentence)

      # 5. në²ˆì§¸ ë°©ë¬¸ ì •ë³´ ì œê±°
      clean_sentence = re.sub(r'\d+ë²ˆì§¸ ë°©ë¬¸', '', clean_sentence)

      # ê³µë°± ì •ë¦¬
      clean_sentence = clean_sentence.strip()

      if clean_sentence:  # ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì¶”ê°€
        # ë¬¸ì¥ ë’¤ì— êµ¬ë‘ì  ì¶”ê°€ (ì›ë˜ ë¬¸ì¥ ëì— ìˆë˜ êµ¬ë‘ì ì´ ë¶„ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ)
        clean_sentence = clean_sentence.strip() + "."
        keyword_sentences.append(clean_sentence)

  return keyword_sentences


def is_food_related(keyword):
  """
  í‚¤ì›Œë“œê°€ ìŒì‹ ê´€ë ¨ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜

  Args:
      keyword (str): í™•ì¸í•  í‚¤ì›Œë“œ

  Returns:
      bool: ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œë©´ True, ì•„ë‹ˆë©´ False
  """
  # ì œì™¸í•  í‚¤ì›Œë“œ ëª©ë¡ì— ìˆëŠ” ê²½ìš° False ë°˜í™˜
  if keyword in EXCLUDED_KEYWORDS:
    return False

  # í‚¤ì›Œë“œê°€ FOOD_KEYWORDS ì§‘í•©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
  for food_keyword in FOOD_KEYWORDS:
    if food_keyword in keyword or keyword in food_keyword:
      return True
  return False


def identify_review_source(review, review_text, default_source, hdfs_directory):
  """
  ë¦¬ë·° ì†ŒìŠ¤ë¥¼ ì‹ë³„í•˜ëŠ” í•¨ìˆ˜

  Args:
      review (dict): ë¦¬ë·° ë°ì´í„°
      review_text (str): ë¦¬ë·° í…ìŠ¤íŠ¸
      default_source (str): ê¸°ë³¸ ì†ŒìŠ¤ (í”Œë ˆì´ìŠ¤ ë˜ëŠ” ë¸”ë¡œê·¸)
      hdfs_directory (str): HDFS ë””ë ‰í† ë¦¬ ê²½ë¡œ

  Returns:
      str: ì‹ë³„ëœ ì†ŒìŠ¤ (ë¸”ë¡œê·¸ ë˜ëŠ” í”Œë ˆì´ìŠ¤)
  """
  # ì´ë¯¸ ì†ŒìŠ¤ê°€ ì§€ì •ë˜ì–´ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
  if 'source' in review and review['source']:
    return review['source']

  # êµ¬ê¸€ ë¦¬ë·° ë””ë ‰í† ë¦¬ í™•ì¸
  if "google_review" in hdfs_directory:
    return "êµ¬ê¸€"

  # ê²½ë¡œì— ugc_reviewê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ë¸”ë¡œê·¸ë¡œ ì„¤ì •
  if "ugc_review" in hdfs_directory:
    return "ë¸”ë¡œê·¸"

  # ë¸”ë¡œê·¸ ë¦¬ë·° ë””ë ‰í† ë¦¬ì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ëŠ” ë¸”ë¡œê·¸ë¡œ ìœ ì§€
  if default_source == "ë¸”ë¡œê·¸":
    return "ë¸”ë¡œê·¸"

  # íŠ¹ì • ë¸”ë¡œê·¸ ë¦¬ë·° íŒ¨í„´ì´ë‚˜ íŠ¹ì§• í™•ì¸ (ì˜ˆ: ì œëª© í˜•íƒœ, íŠ¹ì • ë¬¸êµ¬ ë“±)
  if "ë¸”ë¡œê·¸" in review_text or "í¬ìŠ¤íŒ…" in review_text or "ë°©ë¬¸ í›„ê¸°" in review_text:
    return "ë¸”ë¡œê·¸"

  # ê·¸ ì™¸ì—ëŠ” ê¸°ë³¸ ì†ŒìŠ¤ ì‚¬ìš©
  return default_source


def remove_first_line_from_blog_review(content, source):
  """
  ë¸”ë¡œê·¸ ë¦¬ë·°ì¸ ê²½ìš° ì²« ì¤„ì„ ì œì™¸í•˜ëŠ” í•¨ìˆ˜

  Args:
      content (str): ë¦¬ë·° ë‚´ìš©
      source (str): ë¦¬ë·° ì†ŒìŠ¤ (ë¸”ë¡œê·¸ ë˜ëŠ” í”Œë ˆì´ìŠ¤)

  Returns:
      str: ì²« ì¤„ì´ ì œì™¸ëœ ë¦¬ë·° ë‚´ìš© (ë¸”ë¡œê·¸ì¸ ê²½ìš°)
  """
  if source == 'ë¸”ë¡œê·¸' and content:
    # ì¤„ë°”ê¿ˆì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
    lines = content.split('\n')
    # ì²« ì¤„ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë‚´ìš© ë°˜í™˜
    if len(lines) > 1:
      return '\n'.join(lines[1:])

  # í”Œë ˆì´ìŠ¤ ë¦¬ë·°ì´ê±°ë‚˜ í•œ ì¤„ì§œë¦¬ ë‚´ìš©ì¸ ê²½ìš° ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
  return content


def count_keyword_in_reviews(reviews, keyword):
  """
  ë¦¬ë·° í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • í‚¤ì›Œë“œê°€ ë“±ì¥í•œ íšŸìˆ˜ë¥¼ ì •í™•íˆ ê³„ì‚°

  Args:
      reviews (list): ë¦¬ë·° ëª©ë¡
      keyword (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ

  Returns:
      int: í‚¤ì›Œë“œê°€ ë“±ì¥í•œ íšŸìˆ˜
  """
  count = 0
  for review in reviews:
    content = review.get('original_content', review.get('content', ''))

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ í‚¤ì›Œë“œ ê°œìˆ˜ ì„¸ê¸°
    sentences = re.split(r'[.!?]\s*', content)
    for sentence in sentences:
      # í‚¤ì›Œë“œê°€ í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°ê° ì„¸ê¸°
      keyword_count_in_sentence = sentence.count(keyword)
      count += keyword_count_in_sentence

  return count


def main(restaurant_ids=None):
  """
  HDFSì—ì„œ JSON íŒŒì¼ì„ ì½ì–´ í‚¤ì›Œë“œ ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
  ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œë§Œ í•„í„°ë§í•˜ê³ , í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë§Œ ì €ì¥í•©ë‹ˆë‹¤.

  Args:
      restaurant_ids (list, range, optional): ì²˜ë¦¬í•  ë ˆìŠ¤í† ë‘ ID ëª©ë¡. ê¸°ë³¸ê°’ì€ None (ì´ ê²½ìš° ID 1ë§Œ ì²˜ë¦¬)
  """
  # HDFS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
  try:
    hdfs_client = HDFSClient()
    print("âœ… HDFS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
  except ValueError as e:
    print(f"âŒ HDFS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    return

  # í‚¤ì›Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
  analyzer = KeywordAnalyzer()
  print("âœ… í‚¤ì›Œë“œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì„±ê³µ")

  # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì´ˆê¸°í™”
  try:
    db_importer = PostgresImporter()
    print("âœ… ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
  except Exception as e:
    print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
    return

  # restaurant_idsê°€ ìˆ«ì í•˜ë‚˜ì¸ ê²½ìš°(ì •ìˆ˜) ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
  if isinstance(restaurant_ids, int):
    restaurant_ids = [restaurant_ids]

  # ì œì™¸ í‚¤ì›Œë“œ ì •ë³´ ì¶œë ¥
  print(f"\n{'=' * 50}")
  print(f"ğŸš« ì œì™¸í•  í‚¤ì›Œë“œ ëª©ë¡: {', '.join(EXCLUDED_KEYWORDS)}")
  print(f"{'=' * 50}")

  # ê° ë ˆìŠ¤í† ë‘ IDì— ëŒ€í•´ ì²˜ë¦¬ ì‹¤í–‰
  for restaurant_id in restaurant_ids:
    # IDë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    restaurant_id_str = str(restaurant_id)
    print(f"\n{'=' * 50}")
    print(f"ğŸ½ï¸ ë ˆìŠ¤í† ë‘ ID: {restaurant_id_str} ì²˜ë¦¬ ì‹œì‘")
    print(f"{'=' * 50}")

    # HDFS ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì • (ë ˆìŠ¤í† ë‘ ë¦¬ë·°, ë¸”ë¡œê·¸ ë¦¬ë·°, êµ¬ê¸€ ë¦¬ë·°)
    hdfs_directories = [
      f"/user/hadoop/add_big_final_rest_review_json/restaurant_id={restaurant_id_str}",
      f"/user/hadoop/add_big_final_rest_ugc_review_json/restaurant_id={restaurant_id_str}",
      f"/user/hadoop/add_big_final_rest_review_google_json/restaurant_id={restaurant_id_str}",
      f"/user/hadoop/add_big_final_rest_review_dining_json/restaurant_id={restaurant_id_str}",
      f"/user/hadoop/add_big_final_rest_review_kakao_json/restaurant_id={restaurant_id_str}"
    ]

    all_processed_reviews = []

    # ë””ë ‰í† ë¦¬ë³„ ì²˜ë¦¬
    for hdfs_directory in hdfs_directories:
      # ë¦¬ë·° ì†ŒìŠ¤ íƒ€ì… ì‹ë³„
      if "google" in hdfs_directory:
        default_source = "êµ¬ê¸€"
      elif "ugc_review" in hdfs_directory:
        default_source = "ë¸”ë¡œê·¸"
      elif "dining" in hdfs_directory:
        default_source = "ë‹¤ì´ë‹ì½”ë“œ"
      elif "kakao" in hdfs_directory:
        default_source = "ì¹´ì¹´ì˜¤"
      else:
        default_source = "í”Œë ˆì´ìŠ¤"

      print(f"ğŸ“‚ {default_source} ë¦¬ë·° ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì‹œì‘: {hdfs_directory}")

      # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
      print(f"ğŸ” ë””ë²„ê¹… - ë””ë ‰í† ë¦¬: {hdfs_directory}, default_source: {default_source}")

      # ë””ë ‰í† ë¦¬ë³„ ì²˜ë¦¬ëœ ë¦¬ë·° ì¹´ìš´íŠ¸ ë³€ìˆ˜ ì´ˆê¸°í™”
      directory_processed_reviews = []

      try:
        # ëª¨ë“  JSON íŒŒì¼ ì¬ê·€ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
        json_files = hdfs_client.get_all_json_files(hdfs_directory)

        if not json_files:
          print(f"âŒ ë””ë ‰í† ë¦¬ '{hdfs_directory}'ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
          continue

        # ê° JSON íŒŒì¼ ì²˜ë¦¬
        for file_path in json_files:
          print(f"ğŸ” íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file_path}")

          try:
            # HDFSì—ì„œ JSON íŒŒì¼ ì½ê¸°
            json_content = hdfs_client.read_file(file_path)

            # JSON íŒŒì‹± (ë¼ì¸ ë‹¨ìœ„ ë˜ëŠ” ì „ì²´ JSON ê°ì²´)
            try:
              # ì „ì²´ JSON ê°ì²´ í˜•ì‹ ì²˜ë¦¬ ì‹œë„
              data = json.loads(json_content)

              # processed_review í•„ë“œì—ì„œ ë¦¬ë·° í…ìŠ¤íŠ¸ ì¶”ì¶œ
              if "processed_review" in data:
                reviews = analyzer.extract_reviews(data["processed_review"])
              # ì¹´ì¹´ì˜¤ ë¦¬ë·° í˜•ì‹ ì²˜ë¦¬
              elif "rest_review_kakao" in data:
                reviews = []
                for review_text in data["rest_review_kakao"]:
                  if review_text and review_text.strip():
                    reviews.append({"content": review_text, "source": "ì¹´ì¹´ì˜¤"})
              # êµ¬ê¸€ ë¦¬ë·° í˜•ì‹ ì²˜ë¦¬
              elif "rest_review_google" in data:
                reviews = []
                for review_text in data["rest_review_google"]:
                  if review_text and review_text.strip():
                    reviews.append({"content": review_text, "source": "êµ¬ê¸€"})
              # ë‹¤ì´ë‹ì½”ë“œ ë¦¬ë·° í˜•ì‹ ì²˜ë¦¬
              elif "rest_review_dining" in data:
                reviews = []
                for review_text in data["rest_review_dining"]:
                  if review_text and review_text.strip():
                    reviews.append({"content": review_text, "source": "ë‹¤ì´ë‹ì½”ë“œ"})
              else:
                # review_total_text í•„ë“œê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ JSON ì²˜ë¦¬
                reviews = []
                if isinstance(data, list):
                  reviews = data
                elif isinstance(data, dict) and 'reviews' in data:
                  reviews = data['reviews']
                else:
                  reviews = [data]  # ë‹¨ì¼ ë¦¬ë·°ì¸ ê²½ìš°

            except json.JSONDecodeError:
              # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ, ë¼ì¸ ë‹¨ìœ„ ì²˜ë¦¬ ì‹œë„
              print("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ë¼ì¸ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
              reviews = []
              for line in json_content.strip().split('\n'):
                if line.strip():
                  try:
                    review = json.loads(line)
                    reviews.append(review)
                  except json.JSONDecodeError:
                    continue

            print(f"ğŸ“Š ì´ {len(reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ íŒŒì‹±í–ˆìŠµë‹ˆë‹¤.")

            # ë¦¬ë·°ì— 'content' í•„ë“œê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            valid_reviews = []
            for review in reviews:
              if isinstance(review, dict):
                # 'content' í•„ë“œê°€ ì—†ìœ¼ë©´ 'text' ë˜ëŠ” 'review_text' í•„ë“œ ì°¾ê¸°
                if 'content' not in review:
                  for field in ['text', 'review_text', 'review', 'comment']:
                    if field in review:
                      review['content'] = review[field]
                      break

                # ì—¬ì „íˆ 'content' í•„ë“œê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if 'content' in review:
                  # ë¦¬ë·° ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€ (ìˆ˜ì •ëœ í•¨ìˆ˜ë¡œ ì†ŒìŠ¤ ì‹ë³„)
                  review['source'] = identify_review_source(
                      review,
                      review['content'],
                      default_source,
                      hdfs_directory  # hdfs_directory íŒŒë¼ë¯¸í„° ì¶”ê°€
                  )

                  # ë¸”ë¡œê·¸ ë¦¬ë·°ì¸ ê²½ìš° ì²« ì¤„ ì œì™¸í•˜ê¸°
                  if review['source'] == 'ë¸”ë¡œê·¸':
                    original_content = review['content']
                    review['content'] = remove_first_line_from_blog_review(
                        original_content, 'ë¸”ë¡œê·¸')

                  # ì›ë³¸ ë‚´ìš© ì €ì¥ (ë‚˜ì¤‘ì— ë¬¸ì¥ ì¶”ì¶œìš©)
                  review['original_content'] = review['content']

                  valid_reviews.append(review)

            # í‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰
            processed_reviews = analyzer.process_reviews(valid_reviews)
            directory_processed_reviews.extend(processed_reviews)
            all_processed_reviews.extend(processed_reviews)

          except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

        # ë””ë ‰í† ë¦¬ë³„ ì²˜ë¦¬ëœ ë¦¬ë·° ìˆ˜ ì¶œë ¥
        review_count = len(directory_processed_reviews)
        print(
            f"ğŸ“Š ë””ë ‰í† ë¦¬ '{hdfs_directory}'ì—ì„œ ì²˜ë¦¬ëœ ë¦¬ë·° ìˆ˜: {review_count}")

        # ì†ŒìŠ¤ë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ ë¦¬ë·° ìˆ˜ ì¶œë ¥
        blog_reviews = [r for r in directory_processed_reviews if
                        r.get('source') == 'ë¸”ë¡œê·¸']
        place_reviews = [r for r in directory_processed_reviews if
                         r.get('source') == 'í”Œë ˆì´ìŠ¤']
        google_reviews = [r for r in directory_processed_reviews if
                          r.get('source') == 'êµ¬ê¸€']
        dining_reviews = [r for r in directory_processed_reviews if
                          r.get('source') == 'ë‹¤ì´ë‹ì½”ë“œ']
        kakao_reviews = [r for r in directory_processed_reviews if
                         r.get('source') == 'ì¹´ì¹´ì˜¤']

        print(f"ğŸ“ í”Œë ˆì´ìŠ¤ ë¦¬ë·° ì²˜ë¦¬ ìˆ˜: {len(place_reviews)}")
        print(f"ğŸ“ ë¸”ë¡œê·¸ ë¦¬ë·° ì²˜ë¦¬ ìˆ˜: {len(blog_reviews)}")
        print(f"ğŸ“ êµ¬ê¸€ ë¦¬ë·° ì²˜ë¦¬ ìˆ˜: {len(google_reviews)}")
        print(f"ğŸ“ ë‹¤ì´ë‹ì½”ë“œ ë¦¬ë·° ì²˜ë¦¬ ìˆ˜: {len(dining_reviews)}")
        print(f"ğŸ“ ì¹´ì¹´ì˜¤ ë¦¬ë·° ì²˜ë¦¬ ìˆ˜: {len(kakao_reviews)}")

      except Exception as e:
        print(f"âŒ ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if all_processed_reviews:
      # ëª¨ë“  ì²˜ë¦¬ëœ ë¦¬ë·°ì— ëŒ€í•œ ë¶„ì„ ì‹¤í–‰
      print(f"ğŸ“Š ì´ {len(all_processed_reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...")

      # ì†ŒìŠ¤ë³„ ë¦¬ë·° ìˆ˜ ì¶œë ¥
      blog_reviews = [r for r in all_processed_reviews if
                      r.get('source') == 'ë¸”ë¡œê·¸']
      normal_reviews = [r for r in all_processed_reviews if
                        r.get('source') == 'í”Œë ˆì´ìŠ¤']
      google_reviews = [r for r in all_processed_reviews if
                        r.get('source') == 'êµ¬ê¸€']

      print(f"ğŸ“Š í”Œë ˆì´ìŠ¤ ë¦¬ë·° ìˆ˜: {len(normal_reviews)}")
      print(f"ğŸ“Š ë¸”ë¡œê·¸ ë¦¬ë·° ìˆ˜: {len(blog_reviews)}")
      print(f"ğŸ“Š êµ¬ê¸€ ë¦¬ë·° ìˆ˜: {len(google_reviews)}")

      # ìƒìœ„ í‚¤ì›Œë“œ ë¶„ì„ (ë” ë§ì´ ì¶”ì¶œí•´ì„œ ë‚˜ì¤‘ì— í•„í„°ë§í•  ìˆ˜ ìˆë„ë¡)
      top_n = 50  # ë” ë§ì´ ì¶”ì¶œí•´ì„œ ë‚˜ì¤‘ì— ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œë§Œ í•„í„°ë§
      reviews_per_keyword = 20  # ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 20ê°œ ë¦¬ë·°

      analysis_results = analyzer.analyze_reviews(
          all_processed_reviews,
          top_n=top_n,
          reviews_per_keyword=reviews_per_keyword
      )

      # ë³´ê³ ì„œ ìƒì„±
      report = analyzer.generate_report(analysis_results)

      # ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œë§Œ í•„í„°ë§ ë° í‚¤ì›Œë“œë³„ ì •í™•í•œ ì¹´ìš´íŠ¸ ê³„ì‚°
      food_keywords = []
      for keyword, _ in analysis_results['top_keywords']:
        if is_food_related(keyword):
          # ê° í‚¤ì›Œë“œë³„ ì •í™•í•œ ë“±ì¥ íšŸìˆ˜ ë‹¤ì‹œ ê³„ì‚°
          accurate_count = count_keyword_in_reviews(all_processed_reviews,
                                                    keyword)
          food_keywords.append((keyword, accurate_count))

      # íšŸìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
      food_keywords.sort(key=lambda x: x[1], reverse=True)

      # ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œë§Œ ë‚¨ê¸°ê¸°
      analysis_results['top_keywords'] = food_keywords

      # ê²°ê³¼ ì¶œë ¥
      print("\n===== ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ =====")
      print(f"ì´ ë¦¬ë·° ìˆ˜: {report['summary']['total_reviews']}")
      print(f"ì œì™¸ëœ í‚¤ì›Œë“œ: {', '.join(EXCLUDED_KEYWORDS)}")

      print("\n===== ìƒìœ„ ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œ =====")
      for keyword, count in food_keywords:
        print(f"'{keyword}': {count}íšŒ")

      # ê²°ê³¼ë¥¼ ë¡œì»¬ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
      output_directory = "results"

      # ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
      if not os.path.exists(output_directory):
        os.makedirs(output_directory)

      # ë ˆìŠ¤í† ë‘ IDë¡œ íŒŒì¼ëª… ìƒì„±
      result_filename = f"{output_directory}/restaurant_{restaurant_id_str}_food_keywords.txt"

      try:
        # ê²°ê³¼ë¥¼ ë¡œì»¬ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        with open(result_filename, 'w', encoding='utf-8') as f:
          f.write(f"===== ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ =====\n")
          f.write(f"ë ˆìŠ¤í† ë‘ ID: {restaurant_id_str}\n")
          f.write(f"ì´ ë¦¬ë·° ìˆ˜: {analysis_results['total_reviews']}\n")
          f.write(f"ì œì™¸ëœ í‚¤ì›Œë“œ: {', '.join(EXCLUDED_KEYWORDS)}\n\n")

          f.write(f"===== ìƒìœ„ ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œ =====\n")
          for keyword, count in food_keywords:
            f.write(f"'{keyword}': {count}íšŒ\n")

          f.write("\n===== í‚¤ì›Œë“œë³„ ê´€ë ¨ ë¬¸ì¥ (ìµœëŒ€ 20ê°œ) =====\n")
          for keyword, count in food_keywords:
            f.write(f"\n## '{keyword}' ({count}íšŒ) ##\n")

            # í•´ë‹¹ í‚¤ì›Œë“œì˜ ë¦¬ë·° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            keyword_data = analysis_results['keyword_reviews'].get(keyword, {})
            reviews = keyword_data.get('reviews', [])

            # ê° ë¦¬ë·°ì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë§Œ ì¶”ì¶œ (ì†ŒìŠ¤ë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ ì €ì¥)
            place_sentences = []
            blog_sentences = []
            google_sentences = []
            dining_sentences = []
            kakao_sentences = []

            for i, review in enumerate(reviews, 1):
              content = review.get('original_content',
                                   review.get('content', ''))
              source = review.get('source', 'í”Œë ˆì´ìŠ¤')

              # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì¶”ì¶œ
              sentences = extract_sentence_with_keyword(content, keyword)

              # ì†ŒìŠ¤ë³„ë¡œ ë¬¸ì¥ ë¶„ë¥˜
              for sentence in sentences:
                if source == 'í”Œë ˆì´ìŠ¤':
                  place_sentences.append((source, sentence))
                elif source == 'ë¸”ë¡œê·¸':
                  blog_sentences.append((source, sentence))
                elif source == 'êµ¬ê¸€':
                  google_sentences.append((source, sentence))
                elif source == 'ë‹¤ì´ë‹ì½”ë“œ':
                  dining_sentences.append((source, sentence))
                elif source == 'ì¹´ì¹´ì˜¤':
                  kakao_sentences.append((source, sentence))

            # ê° ì†ŒìŠ¤ë³„ ìµœëŒ€ ë¬¸ì¥ ìˆ˜ ê³„ì‚° (ìµœëŒ€í•œ ê· ë“±í•˜ê²Œ ë¶„ë°°)
            sources = [place_sentences, blog_sentences, google_sentences,
                       dining_sentences, kakao_sentences]
            active_sources = [s for s in sources if len(s) > 0]
            total_sources = len(active_sources)
            max_per_source = min(4,
                                 20 // total_sources) if total_sources > 0 else 0

            # ìµœì¢… ì¶œë ¥í•  ë¬¸ì¥ ëª©ë¡ ì¤€ë¹„
            final_sentences = []

            # ê° ì†ŒìŠ¤ì—ì„œ ë¬¸ì¥ ì„ íƒ (ìµœëŒ€ max_per_sourceê°œ)
            if place_sentences:
              final_sentences.extend(place_sentences[:max_per_source])
            if blog_sentences:
              final_sentences.extend(blog_sentences[:max_per_source])
            if google_sentences:
              final_sentences.extend(google_sentences[:max_per_source])
            if dining_sentences:
              final_sentences.extend(dining_sentences[:max_per_source])
            if kakao_sentences:
              final_sentences.extend(kakao_sentences[:max_per_source])

            # ë§Œì•½ 20ê°œê°€ ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ë‚¨ì€ ê³µê°„ì— ê°€ì¥ ë§ì€ ë¬¸ì¥ì´ ìˆëŠ” ì†ŒìŠ¤ì—ì„œ ì¶”ê°€
            remaining_slots = 20 - len(final_sentences)
            if remaining_slots > 0:
              # ì†ŒìŠ¤ë³„ ë‚¨ì€ ë¬¸ì¥ ìˆ˜ í™•ì¸
              remaining_place = max(0, len(place_sentences) - max_per_source)
              remaining_blog = max(0, len(blog_sentences) - max_per_source)
              remaining_google = max(0, len(google_sentences) - max_per_source)
              remaining_dining = max(0, len(dining_sentences) - max_per_source)
              remaining_kakao = max(0, len(kakao_sentences) - max_per_source)

              # ë‚¨ì€ ë¬¸ì¥ì´ ê°€ì¥ ë§ì€ ì†ŒìŠ¤ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¶”ê°€
              sources_with_remaining = []
              if remaining_place > 0:
                sources_with_remaining.append(
                    (remaining_place, place_sentences[max_per_source:], 'í”Œë ˆì´ìŠ¤'))
              if remaining_blog > 0:
                sources_with_remaining.append(
                    (remaining_blog, blog_sentences[max_per_source:], 'ë¸”ë¡œê·¸'))
              if remaining_google > 0:
                sources_with_remaining.append(
                    (remaining_google, google_sentences[max_per_source:], 'êµ¬ê¸€'))
              if remaining_dining > 0:
                sources_with_remaining.append((remaining_dining,
                                               dining_sentences[
                                               max_per_source:], 'ë‹¤ì´ë‹ì½”ë“œ'))
              if remaining_kakao > 0:
                sources_with_remaining.append(
                    (remaining_kakao, kakao_sentences[max_per_source:], 'ì¹´ì¹´ì˜¤'))
              sources_with_remaining.sort(reverse=True)  # ë‚¨ì€ ë¬¸ì¥ì´ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬

              # ë‚¨ì€ ìë¦¬ë¥¼ ì±„ì›€
              slots_filled = 0
              for remaining, sentences, source_type in sources_with_remaining:
                if slots_filled >= remaining_slots:
                  break

                to_add = min(remaining, remaining_slots - slots_filled)
                final_sentences.extend(sentences[:to_add])
                slots_filled += to_add

            # ìµœì¢… ì„ íƒëœ ë¬¸ì¥ë“¤ì„ ì¶œë ¥
            for i, (source, sentence) in enumerate(final_sentences, 1):
              f.write(f"{i}. [{source}] {sentence}\n")

        print(f"âœ… ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ê°€ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {result_filename}")

        # JSON í˜•ì‹ìœ¼ë¡œ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ ì €ì¥
        # ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œ ë°°ì—´ í˜•íƒœë¡œ ì €ì¥
        keywords_array = []
        for keyword, count in food_keywords:
          keyword_obj = {
            'keyword': keyword,
            'count': count,
            'sentences': []
          }

          # í•´ë‹¹ í‚¤ì›Œë“œì˜ ë¦¬ë·° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
          keyword_data = analysis_results['keyword_reviews'].get(keyword, {})
          reviews = keyword_data.get('reviews', [])

          # ê° ë¦¬ë·°ì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì¶”ì¶œ (ì†ŒìŠ¤ë³„ë¡œ ë¶„ë¥˜)
          place_sentences = []
          blog_sentences = []
          google_sentences = []
          dining_sentences = []
          kakao_sentences = []

          for review in reviews:
            content = review.get('original_content', review.get('content', ''))
            source = review.get('source', 'í”Œë ˆì´ìŠ¤')

            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì¶”ì¶œ
            sentences = extract_sentence_with_keyword(content, keyword)

            # ì†ŒìŠ¤ë³„ë¡œ ë¬¸ì¥ ë¶„ë¥˜
            for sentence in sentences:
              sentence_obj = {
                'content': sentence,
                'source': source
              }

              if source == 'í”Œë ˆì´ìŠ¤':
                place_sentences.append(sentence_obj)
              elif source == 'ë¸”ë¡œê·¸':
                blog_sentences.append(sentence_obj)
              elif source == 'êµ¬ê¸€':
                google_sentences.append(sentence_obj)
              elif source == 'ë‹¤ì´ë‹ì½”ë“œ':
                dining_sentences.append(sentence_obj)
              elif source == 'ì¹´ì¹´ì˜¤':
                kakao_sentences.append(sentence_obj)

          # ê° ì†ŒìŠ¤ë³„ ìµœëŒ€ ë¬¸ì¥ ìˆ˜ ê³„ì‚° (ìµœëŒ€í•œ ê· ë“±í•˜ê²Œ ë¶„ë°°)
          sources = [place_sentences, blog_sentences, google_sentences,
                     dining_sentences, kakao_sentences]
          active_sources = [s for s in sources if len(s) > 0]
          total_sources = len(active_sources)
          max_per_source = min(4,
                               20 // total_sources) if total_sources > 0 else 0

          # ê° ì†ŒìŠ¤ì—ì„œ ë¬¸ì¥ ì„ íƒ
          selected_sentences = []
          if place_sentences:
            selected_sentences.extend(place_sentences[:max_per_source])
          if blog_sentences:
            selected_sentences.extend(blog_sentences[:max_per_source])
          if google_sentences:
            selected_sentences.extend(google_sentences[:max_per_source])
          if dining_sentences:
            selected_sentences.extend(dining_sentences[:max_per_source])
          if kakao_sentences:
            selected_sentences.extend(kakao_sentences[:max_per_source])

          # ë§Œì•½ 20ê°œê°€ ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ë‚¨ì€ ê³µê°„ì— ê°€ì¥ ë§ì€ ë¬¸ì¥ì´ ìˆëŠ” ì†ŒìŠ¤ì—ì„œ ì¶”ê°€
          # ë§Œì•½ 20ê°œê°€ ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ë‚¨ì€ ê³µê°„ì— ê°€ì¥ ë§ì€ ë¬¸ì¥ì´ ìˆëŠ” ì†ŒìŠ¤ì—ì„œ ì¶”ê°€
          remaining_slots = 20 - len(selected_sentences)
          if remaining_slots > 0:
            # ì†ŒìŠ¤ë³„ ë‚¨ì€ ë¬¸ì¥ ìˆ˜ í™•ì¸
            remaining_place = max(0, len(place_sentences) - max_per_source)
            remaining_blog = max(0, len(blog_sentences) - max_per_source)
            remaining_google = max(0, len(google_sentences) - max_per_source)
            remaining_dining = max(0, len(dining_sentences) - max_per_source)
            remaining_kakao = max(0, len(kakao_sentences) - max_per_source)

            # ë‚¨ì€ ë¬¸ì¥ì´ ê°€ì¥ ë§ì€ ì†ŒìŠ¤ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¶”ê°€
            sources_with_remaining = []
            if remaining_place > 0:
              sources_with_remaining.append(
                  (remaining_place, place_sentences[max_per_source:]))
            if remaining_blog > 0:
              sources_with_remaining.append(
                  (remaining_blog, blog_sentences[max_per_source:]))
            if remaining_google > 0:
              sources_with_remaining.append(
                  (remaining_google, google_sentences[max_per_source:]))
            if remaining_dining > 0:
              sources_with_remaining.append(
                  (remaining_dining, dining_sentences[max_per_source:]))
            if remaining_kakao > 0:
              sources_with_remaining.append(
                  (remaining_kakao, kakao_sentences[max_per_source:]))
            sources_with_remaining.sort(key=lambda x: x[0],
                                        reverse=True)  # ë‚¨ì€ ë¬¸ì¥ì´ ë§ì€ ìˆœìœ¼ë¡œ ì •ë ¬

            # ë‚¨ì€ ìë¦¬ë¥¼ ì±„ì›€
            slots_filled = 0
            for remaining, sentences in sources_with_remaining:
              if slots_filled >= remaining_slots:
                break

              to_add = min(remaining, remaining_slots - slots_filled)
              selected_sentences.extend(sentences[:to_add])
              slots_filled += to_add

          # ìµœì¢… ì„ íƒëœ ë¬¸ì¥ë“¤ì„ í‚¤ì›Œë“œ ê°ì²´ì— ì¶”ê°€
          keyword_obj['sentences'] = selected_sentences[:20]  # ìµœëŒ€ 20ê°œë¡œ ì œí•œ

          keywords_array.append(keyword_obj)

        json_filename = f"{output_directory}/restaurant_{restaurant_id_str}_food_keywords.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
          json.dump(keywords_array, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSON í˜•ì‹ì˜ ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {json_filename}")

        # ë°ì´í„°ë² ì´ìŠ¤ì— ê²°ê³¼ ì €ì¥
        save_keyword_results_to_db(db_importer, restaurant_id_str,
                                   analysis_results, food_keywords)

      except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
      print(f"âŒ ë ˆìŠ¤í† ë‘ ID {restaurant_id_str}ì— ëŒ€í•œ ë¶„ì„í•  ìœ íš¨í•œ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")

  # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ
  db_importer.close()


# í‚¤ì›Œë“œë³„ ë¬¸ì¥ ì²˜ë¦¬ ë° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í•¨ìˆ˜ ìˆ˜ì •
import json
import os
from src.hadoop.hdfs_client import HDFSClient
from src.keyword.analysis import KeywordAnalyzer
from dotenv import load_dotenv
from db.database import PostgresImporter
import re


# ê¸°ì¡´ ì½”ë“œ ë™ì¼ (FOOD_KEYWORDS, EXCLUDED_KEYWORDS ë“±)

# ê¸°ì¡´ì˜ í•¨ìˆ˜ë“¤ ê·¸ëŒ€ë¡œ ìœ ì§€ (extract_sentence_with_keyword, is_food_related ë“±)

def save_keyword_results_to_db(db_importer, restaurant_id, analysis_results,
    food_keywords):
  """
  ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
  ê° í‚¤ì›Œë“œë³„ë¡œ í¬í•¨ëœ ë¬¸ì¥ë§Œ ì €ì¥í•©ë‹ˆë‹¤.
  ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ ì €ì¥í•©ë‹ˆë‹¤.
  """
  try:
    print(f"\n===== ë°ì´í„°ë² ì´ìŠ¤ì— ìƒìœ„ 5ê°œ ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œ ë° ë¬¸ì¥ ì €ì¥ =====")

    # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ ì„ íƒ
    top_5_keywords = food_keywords[:5]
    if len(top_5_keywords) < 5:
      print(f"âš ï¸ í‚¤ì›Œë“œê°€ 5ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ {len(top_5_keywords)}ê°œì˜ í‚¤ì›Œë“œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    print(f"ğŸ“Š ì €ì¥í•  ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ: {', '.join([kw for kw, _ in top_5_keywords])}")

    # ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ ì„¤ì • (ì¹´ì¹´ì˜¤, êµ¬ê¸€, ë‹¤ì´ë‹ì½”ë“œ, í”Œë ˆì´ìŠ¤, ë¸”ë¡œê·¸ ìˆœ)
    source_priority_order = ['ì¹´ì¹´ì˜¤', 'êµ¬ê¸€', 'ë‹¤ì´ë‹ì½”ë“œ', 'í”Œë ˆì´ìŠ¤', 'ë¸”ë¡œê·¸']

    def select_comprehensive_sentences(sources, max_total_sentences=20):
      # ëª¨ë“  í™œì„± ì†ŒìŠ¤ ìˆ˜ì§‘
      all_sentences = []
      source_sentence_counts = {source: 0 for source in
                                ['ì¹´ì¹´ì˜¤', 'êµ¬ê¸€', 'ë‹¤ì´ë‹ì½”ë“œ', 'í”Œë ˆì´ìŠ¤', 'ë¸”ë¡œê·¸']}

      # ê° ì†ŒìŠ¤ë³„ ìµœëŒ€ ë¬¸ì¥ ìˆ˜ (ë™ì ìœ¼ë¡œ ì¡°ì •)
      source_max_sentences = {
        'ì¹´ì¹´ì˜¤': max_total_sentences // 5 + 2,
        'êµ¬ê¸€': max_total_sentences // 5 + 2,
        'ë‹¤ì´ë‹ì½”ë“œ': max_total_sentences // 5 + 2,
        'í”Œë ˆì´ìŠ¤': max_total_sentences // 5 + 2,
        'ë¸”ë¡œê·¸': max_total_sentences // 5 + 2
      }

      # ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ (ì¹´ì¹´ì˜¤, êµ¬ê¸€, ë‹¤ì´ë‹ì½”ë“œ, í”Œë ˆì´ìŠ¤, ë¸”ë¡œê·¸)
      source_priority_order = ['ì¹´ì¹´ì˜¤', 'êµ¬ê¸€', 'ë‹¤ì´ë‹ì½”ë“œ', 'í”Œë ˆì´ìŠ¤', 'ë¸”ë¡œê·¸']

      # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì§‘í•©
      seen_sentences = set()

      # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ë¬¸ì¥ ìˆ˜ì§‘
      for source in source_priority_order:
        if source in sources and sources[source]:
          for sentence, src in sources[source]:
            # ì¤‘ë³µ ë¬¸ì¥ ì œì™¸
            if sentence not in seen_sentences:
              # í•´ë‹¹ ì†ŒìŠ¤ì˜ ë¬¸ì¥ ìˆ˜ê°€ ìµœëŒ€ì¹˜ì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ì¶”ê°€
              if source_sentence_counts[source] < source_max_sentences[source]:
                all_sentences.append((sentence, src))
                source_sentence_counts[source] += 1
                seen_sentences.add(sentence)

            # ì´ ë¬¸ì¥ ìˆ˜ê°€ ìµœëŒ€ì¹˜ì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ
            if len(all_sentences) >= max_total_sentences:
              break

          # ì´ ë¬¸ì¥ ìˆ˜ê°€ ìµœëŒ€ì¹˜ì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ
          if len(all_sentences) >= max_total_sentences:
            break

      # ì—¬ì „íˆ ë¬¸ì¥ ìˆ˜ê°€ ë¶€ì¡±í•˜ë‹¤ë©´ ë‚˜ë¨¸ì§€ ì†ŒìŠ¤ì—ì„œ ë³´ì¶©
      if len(all_sentences) < max_total_sentences:
        for source in sources:
          if source not in source_priority_order:
            for sentence, src in sources[source]:
              if sentence not in seen_sentences:
                all_sentences.append((sentence, src))
                seen_sentences.add(sentence)

              if len(all_sentences) >= max_total_sentences:
                break

            if len(all_sentences) >= max_total_sentences:
              break

      return all_sentences[:max_total_sentences]

    # 1. store_keyword í…Œì´ë¸”ì— í‚¤ì›Œë“œ ì €ì¥
    keyword_id_map = {}  # í‚¤ì›Œë“œì™€ DBì— ì €ì¥ëœ ID ë§¤í•‘ì„ ì €ì¥í•  ì‚¬ì „

    # ìƒìœ„ 5ê°œ ìŒì‹ ê´€ë ¨ í‚¤ì›Œë“œë§Œ ì²˜ë¦¬
    for keyword, count in top_5_keywords:
      # í‚¤ì›Œë“œ ì €ì¥ ë° ìƒì„±ëœ ID ê°€ì ¸ì˜¤ê¸°
      keyword_id = db_importer._save_keyword(keyword, count, restaurant_id)
      keyword_id_map[keyword] = keyword_id
      print(
        f"âœ… í‚¤ì›Œë“œ '{keyword}' (ë¹ˆë„: {count})ê°€ store_keyword í…Œì´ë¸”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ID: {keyword_id}")

    # 2. ê° í‚¤ì›Œë“œë³„ ë¦¬ë·°ì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë§Œ keyword_review í…Œì´ë¸”ì— ì €ì¥
    sentence_count = 0
    for keyword, count in top_5_keywords:
      # í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” ID ê°€ì ¸ì˜¤ê¸°
      keyword_id = keyword_id_map.get(keyword)
      if not keyword_id:
        print(f"âš ï¸ í‚¤ì›Œë“œ '{keyword}'ì˜ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        continue

      # í•´ë‹¹ í‚¤ì›Œë“œì˜ ë¦¬ë·° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
      keyword_data = analysis_results['keyword_reviews'].get(keyword, {})
      reviews = keyword_data.get('reviews', [])

      # ê° ë¦¬ë·°ì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì¶”ì¶œ (ì†ŒìŠ¤ë³„ë¡œ ë¶„ë¥˜)
      sources_sentences = {
        'ì¹´ì¹´ì˜¤': [],
        'êµ¬ê¸€': [],
        'ë‹¤ì´ë‹ì½”ë“œ': [],
        'í”Œë ˆì´ìŠ¤': [],
        'ë¸”ë¡œê·¸': []
      }

      for review in reviews:
        content = review.get('original_content', review.get('content', ''))
        source = review.get('source', 'í”Œë ˆì´ìŠ¤')

        # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì¶”ì¶œ
        sentences = extract_sentence_with_keyword(content, keyword)

        # ì†ŒìŠ¤ë³„ë¡œ ë¬¸ì¥ ë¶„ë¥˜ (ì¡´ì¬í•˜ëŠ” ì†ŒìŠ¤ë§Œ)
        if source in sources_sentences:
          for sentence in sentences:
            sources_sentences[source].append((sentence, source))

      # ë¬¸ì¥ ì„ íƒ
      selected_sentences = select_comprehensive_sentences(
          {source: sentences for source, sentences in sources_sentences.items()
           if sentences},
          max_total_sentences=20
      )

      # ì„ íƒëœ ë¬¸ì¥ì˜ ì†ŒìŠ¤ ë¶„í¬ ì¶œë ¥
      source_distribution = {}
      for sentence, source in selected_sentences:
        source_distribution[source] = source_distribution.get(source, 0) + 1

      print(f"\ní‚¤ì›Œë“œ '{keyword}'ì˜ ì„ íƒëœ ë¬¸ì¥ ì†ŒìŠ¤ ë¶„í¬:")
      for source, count in sorted(source_distribution.items()):
        print(f"{source}: {count}ê°œ")

      # ìµœì¢… ì„ íƒëœ ë¬¸ì¥ë“¤ì„ DBì— ì €ì¥
      sentences_saved = 0
      for sentence, source in selected_sentences:
        if sentences_saved >= 20:  # ìµœëŒ€ 20ê°œ ë¬¸ì¥ë§Œ ì €ì¥
          break

        # ë¬¸ì¥ ì €ì¥ (ë¦¬ë·° ë‚´ìš© ëŒ€ì‹  ë¬¸ì¥ ì €ì¥)
        relation_id = db_importer._save_review_keyword_relation_with_source(
            sentence, keyword_id, source)
        if relation_id > 0:
          sentences_saved += 1
          sentence_count += 1

      print(f"âœ… í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•´ {sentences_saved}ê°œì˜ ë¬¸ì¥ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    print(f"âœ… ì´ {sentence_count}ê°œì˜ ë¬¸ì¥ì´ keyword_review í…Œì´ë¸”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return True

  except Exception as e:
    print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return False

if __name__ == "__main__":
  # ì²˜ë¦¬í•  ë ˆìŠ¤í† ë‘ ID ë²”ìœ„ ì„¤ì •
  start_id = 42
  end_id = 521

  # ë ˆìŠ¤í† ë‘ ID ëª©ë¡ (ë²”ìœ„ ë˜ëŠ” íŠ¹ì • ID ëª©ë¡ ì‚¬ìš© ê°€ëŠ¥)
  restaurant_ids = range(start_id, end_id + 1)

  main(restaurant_ids)