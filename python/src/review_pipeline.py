from pipeline.pipeline import Pipeline
from db.database import PostgresImporter
import time
import os
from collections import Counter
import json
import re
import subprocess

# ì²˜ë¦¬í•  ë ˆìŠ¤í† ë‘ ID ë²”ìœ„
start_id = 9
end_id = 9

# ë ˆìŠ¤í† ë‘ ID ëª©ë¡ (ë²”ìœ„ ë˜ëŠ” íŠ¹ì • ID ëª©ë¡ ì‚¬ìš© ê°€ëŠ¥)
restaurant_ids = range(start_id, end_id + 1)  # 1ë¶€í„° 10ê¹Œì§€
# restaurant_ids = [2, 5, 10, 15, 20]  # íŠ¹ì • IDë§Œ ì²˜ë¦¬í•˜ë ¤ë©´ ì´ë ‡ê²Œ ë¦¬ìŠ¤íŠ¸ë¡œ ì§€ì •

# ì‹¤í–‰ ëª¨ë“œ ì„¤ì •
extract_mode = True  # True: HDFSì—ì„œ ë¦¬ë·° ì¶”ì¶œ, False: ì €ì¥ëœ íŒŒì¼ì—ì„œ DB ì €ì¥

# ì €ì¥í•  ë°ì´í„° ì œí•œ ì„¤ì •
MAX_KEYWORDS = 5  # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ ì €ì¥
MAX_REVIEWS = 3  # ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 3ê°œ ë¦¬ë·°ë§Œ ì €ì¥

# ë°ì´í„° ì •ì œë¥¼ ìœ„í•œ íŒŒì´í”„ë¼ì¸ ìƒì„±
pipeline = Pipeline()

# Ollama ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - ì¼ë°˜ ë¦¬ë·°ìš©
system_prompt_naver = '''
  í•­ìƒ í•œêµ­ì–´ë¡œ ëŒ€ë‹µ ë¶€íƒë“œë¦½ë‹ˆë‹¤.
  ë‹¤ìŒ ë¦¬ë·° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í‘œì¤€í™”ëœ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

  ### ì‘ì—… ë‚´ìš©:
  1. ê° ë¦¬ë·°ì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ë§Œ ì¶”ì¶œí•˜ì„¸ìš”. ì—¬ëŸ¬ í‚¤ì›Œë“œê°€ ìˆë”ë¼ë„ ê°€ì¥ ì¤‘ìš”í•œ í•˜ë‚˜ë§Œ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.

  2. ë¦¬ë·° ë‚´ìš©ì—ì„œ ìŒì‹ì´ë¦„(ë©”ë‰´ëª…)ì´ ì–¸ê¸‰ë˜ë©´ ë¬´ì¡°ê±´ í•´ë‹¹ ë©”ë‰´ëª…ì„ í‚¤ì›Œë“œë¡œ ì„ íƒí•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´:
     - "ê¹€ì¹˜ì°Œê°œê°€ ë§›ìˆì–´ìš”" â†’ í‚¤ì›Œë“œ: "ê¹€ì¹˜ì°Œê°œ"

  3. ë©”ë‰´ëª…ì´ ì—¬ëŸ¬ ê°œ ë‚˜ì˜¨ë‹¤ë©´ ê°€ì¥ ë¨¼ì € ì–¸ê¸‰ëœ ë©”ë‰´ë‚˜ ê°€ì¥ ê°•ì¡°ëœ ë©”ë‰´ë¥¼ í‚¤ì›Œë“œë¡œ ì„ íƒí•˜ì„¸ìš”.

  4. ë¹„ìŠ·í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œì¤€í™”í•˜ì„¸ìš”(ë©”ë‰´ëª…ì€ ê·¸ëŒ€ë¡œ ìœ ì§€):
     - 'ë§›', 'ë§›ìˆëŠ”', 'ë§›ë‚˜' â†’ 'ë§›ìˆìŒ'
     - 'ê°€ì„±ë¹„', 'ì €ë ´', 'ì‹¸ë‹¤' â†’ 'ê°€ì„±ë¹„'
     - 'ì¹œì ˆ', 'ì¹œì ˆí•œ' â†’ 'ì¹œì ˆí•¨'
     - 'ê¹”ë”', 'ì²­ê²°' â†’ 'ê¹”ë”í•¨'
     - 'ì–‘ ë§ìŒ', 'í‘¸ì§' â†’ 'ì–‘ë§ìŒ'
     - 'í˜¼ë°¥', 'í˜¼ë°¥í•˜ê¸°ì¢‹ì•„í•¨' â†’ 'í˜¼ë°¥'
     - 'ì‹ ì„ í•¨', 'ì‹ ì„ í•œ' â†’ 'ì‹ ì„ í•¨'

  5. ê° ë¦¬ë·° ê°ì²´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ê°œë³„ JSON ê°ì²´ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
    ë°˜ë“œì‹œ ìˆœìˆ˜í•œ JSONë§Œ ë°˜í™˜í•´ì£¼ì‹œê³ , ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ì¶”ê°€ ì„¤ëª…ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    ë‹¤ë¥¸ ì–¸ì–´ë¡œ ì„¤ëª…ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
  {
    "content": "ë¦¬ë·° ë‚´ìš©",
    "keyword": "í‚¤ì›Œë“œ",
    "source": "naver"
  }

  6. ê° JSON ê°ì²´ëŠ” ì„œë¡œ ë¶„ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ëŒ€ê´„í˜¸([])ë¡œ ë¬¶ì§€ ë§ˆì„¸ìš”.

'''

# Ollama ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - ë¸”ë¡œê·¸ ë¦¬ë·°ìš©
system_prompt_blog = '''
  í•­ìƒ í•œêµ­ì–´ë¡œ ëŒ€ë‹µ ë¶€íƒë“œë¦½ë‹ˆë‹¤.
  ë‹¤ìŒ ë¦¬ë·° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í‘œì¤€í™”ëœ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

  ### ì‘ì—… ë‚´ìš©:
  1. ê° ë¦¬ë·°ì—ì„œ ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ë§Œ ì¶”ì¶œí•˜ì„¸ìš”. ì—¬ëŸ¬ í‚¤ì›Œë“œê°€ ìˆë”ë¼ë„ ê°€ì¥ ì¤‘ìš”í•œ í•˜ë‚˜ë§Œ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.

  2. ë¦¬ë·° ë‚´ìš©ì—ì„œ ë©”ë‰´ì´ë¦„ì´ ì–¸ê¸‰ë˜ë©´ ë¬´ì¡°ê±´ í•´ë‹¹ ë©”ë‰´ëª…ì„ í‚¤ì›Œë“œë¡œ ì„ íƒí•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´:
     - "ê¹€ì¹˜ì°Œê°œê°€ ë§›ìˆì–´ìš”" â†’ í‚¤ì›Œë“œ: "ê¹€ì¹˜ì°Œê°œ"

  3. ë©”ë‰´ëª…ì´ ì—¬ëŸ¬ ê°œ ë‚˜ì˜¨ë‹¤ë©´ ê°€ì¥ ë¨¼ì € ì–¸ê¸‰ëœ ë©”ë‰´ë‚˜ ê°€ì¥ ê°•ì¡°ëœ ë©”ë‰´ë¥¼ í‚¤ì›Œë“œë¡œ ì„ íƒí•˜ì„¸ìš”.

  4. ë¹„ìŠ·í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œì¤€í™”í•˜ì„¸ìš”(ë©”ë‰´ëª…ì€ ê·¸ëŒ€ë¡œ ìœ ì§€):
     - 'ë§›', 'ë§›ìˆëŠ”', 'ë§›ë‚˜' â†’ 'ë§›ìˆìŒ'
     - 'ê°€ì„±ë¹„', 'ì €ë ´', 'ì‹¸ë‹¤' â†’ 'ê°€ì„±ë¹„'
     - 'ì¹œì ˆ', 'ì¹œì ˆí•œ' â†’ 'ì¹œì ˆí•¨'
     - 'ê¹”ë”', 'ì²­ê²°' â†’ 'ê¹”ë”í•¨'
     - 'ì–‘ ë§ìŒ', 'í‘¸ì§' â†’ 'ì–‘ë§ìŒ'
     - 'í˜¼ë°¥', 'í˜¼ë°¥í•˜ê¸°ì¢‹ì•„í•¨' â†’ 'í˜¼ë°¥'
     - 'ì‹ ì„ í•¨', 'ì‹ ì„ í•œ' â†’ 'ì‹ ì„ í•¨'


  5. ê° ë¦¬ë·° ê°ì²´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ê°œë³„ JSON ê°ì²´ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
    ë°˜ë“œì‹œ ìˆœìˆ˜í•œ JSONë§Œ ë°˜í™˜í•´ì£¼ì‹œê³ , ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ì¶”ê°€ ì„¤ëª…ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    ë‹¤ë¥¸ ì–¸ì–´ë¡œ ì„¤ëª…ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
  {
    "content": "ë¦¬ë·° ë‚´ìš©",
    "keyword": "í‚¤ì›Œë“œ",
    "source": "blog"
  }

  6. ê° JSON ê°ì²´ëŠ” ì„œë¡œ ë¶„ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ëŒ€ê´„í˜¸([])ë¡œ ë¬¶ì§€ ë§ˆì„¸ìš”.

'''


# íŒŒì¼ì—ì„œ JSON ê°ì²´ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜
def parse_json_objects(file_content):
  # ì¤‘ê´„í˜¸ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ JSON ê°ì²´ íŒ¨í„´
  pattern = r'(\{[^{}]*\})'

  # ëª¨ë“  JSON ê°ì²´ ì°¾ê¸°
  json_objects = re.findall(pattern, file_content)

  # ê° ê°ì²´ íŒŒì‹±
  reviews = []
  for obj_str in json_objects:
    try:
      review = json.loads(obj_str)
      # 'keyword' ë˜ëŠ” 'keywords' í•„ë“œ í†µì¼
      if 'keywords' in review and not 'keyword' in review:
        review['keyword'] = review['keywords']
      # source í•„ë“œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ 'naver' ì„¤ì • (ê¸°ì¡´ ë°ì´í„° í˜¸í™˜ì„±)
      if 'source' not in review:
        review['source'] = 'naver'
      reviews.append(review)
    except json.JSONDecodeError:
      print(f"ì˜ëª»ëœ JSON í˜•ì‹: {obj_str}")

  return reviews


# ë¸”ë¡œê·¸ ë¦¬ë·° ë””ë ‰í† ë¦¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_blog_author_directories(base_dir):
  """
  ë¸”ë¡œê·¸ ë¦¬ë·° ë‚´ì˜ author ì„œë¸Œë””ë ‰í† ë¦¬ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

  Args:
      base_dir (str): ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (/user/hadoop/big_final_rest_ugc_review_json/restaurant_id=X)

  Returns:
      list: author ë””ë ‰í† ë¦¬ ì „ì²´ ê²½ë¡œ ëª©ë¡
  """
  try:
    # HDFSì—ì„œ ë””ë ‰í† ë¦¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    cmd = f"hadoop fs -ls {base_dir}"
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

    directories = []
    if result.returncode == 0:
      # ì¶œë ¥ íŒŒì‹±
      lines = result.stdout.strip().split('\n')
      for line in lines:
        if 'author=' in line:
          # ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ì¶œ
          parts = line.split()
          if len(parts) >= 8:
            dir_path = parts[-1]  # ë§ˆì§€ë§‰ í•„ë“œê°€ ê²½ë¡œ
            directories.append(dir_path)

    return directories

  except Exception as e:
    print(f"ë¸”ë¡œê·¸ author ë””ë ‰í† ë¦¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    return []


# ë¦¬ë·° ì œí•œ í•¨ìˆ˜ - í‚¤ì›Œë“œë³„ í†µí•© ì¹´ìš´íŠ¸, ì†ŒìŠ¤ ì •ë³´ ìœ ì§€
def limit_reviews_by_keywords(reviews, max_keywords=5,
    max_reviews_per_keyword=3):
  """
  í‚¤ì›Œë“œë³„ë¡œ ë¦¬ë·°ë¥¼ ì œí•œí•˜ê³  ì „ì²´ í‚¤ì›Œë“œ ì¹´ìš´íŠ¸ ìœ ì§€

  Args:
      reviews (list): ëª¨ë“  ë¦¬ë·° ëª©ë¡
      max_keywords (int): ì €ì¥í•  ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜
      max_reviews_per_keyword (int): ê° í‚¤ì›Œë“œë‹¹ ì €ì¥í•  ìµœëŒ€ ë¦¬ë·° ìˆ˜

  Returns:
      tuple: (ì œí•œëœ ë¦¬ë·° ëª©ë¡, í‚¤ì›Œë“œ ì¹´ìš´íŠ¸, ì„ íƒëœ í‚¤ì›Œë“œ ëª©ë¡)
  """
  # í‚¤ì›Œë“œ ì¹´ìš´íŠ¸ (ì†ŒìŠ¤ êµ¬ë¶„ ì—†ì´)
  keyword_counts = Counter()
  for review in reviews:
    keyword = review.get("keyword", "")
    if keyword:
      keyword_counts[keyword] += 1

  # ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ì„ íƒ
  top_keywords = [k for k, v in keyword_counts.most_common(max_keywords)]

  # í‚¤ì›Œë“œë³„ ë¦¬ë·° ì œí•œ (ì†ŒìŠ¤ë³„ë¡œ ë¶„ë¦¬)
  limited_reviews = []
  keyword_review_counts = {
    'naver': {k: 0 for k in top_keywords},
    'blog': {k: 0 for k in top_keywords}
  }

  for review in reviews:
    keyword = review.get("keyword", "")
    source = review.get("source", "naver")

    # ìƒìœ„ í‚¤ì›Œë“œì— í¬í•¨ë˜ê³ , í•´ë‹¹ í‚¤ì›Œë“œ/ì†ŒìŠ¤ì˜ ë¦¬ë·° ìˆ˜ê°€ ì œí•œì„ ë„˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
    if keyword in top_keywords and keyword_review_counts[source][
      keyword] < max_reviews_per_keyword:
      limited_reviews.append(review)
      keyword_review_counts[source][keyword] += 1

  return limited_reviews, keyword_counts, top_keywords


# DB ì €ì¥ í•¨ìˆ˜ - ì†ŒìŠ¤ ì •ë³´ë§Œ ì¶”ê°€
def save_limited_reviews_with_source(importer, limited_reviews, keyword_counts,
    top_keywords, store_id):
  """
  ì œí•œëœ ë¦¬ë·°ë§Œ ì €ì¥í•˜ë˜ ì „ì²´ í‚¤ì›Œë“œ ì¹´ìš´íŠ¸ ì •ë³´ëŠ” ìœ ì§€í•˜ë©° ì†ŒìŠ¤ ì •ë³´ë§Œ ë¦¬ë·°ì— ì¶”ê°€

  Args:
      importer: PostgresImporter ì¸ìŠ¤í„´ìŠ¤
      limited_reviews (list): ì œí•œëœ ë¦¬ë·° ëª©ë¡
      keyword_counts (Counter): ì „ì²´ í‚¤ì›Œë“œ ì¹´ìš´íŠ¸
      top_keywords (list): ì„ íƒëœ ìƒìœ„ í‚¤ì›Œë“œ ëª©ë¡
      store_id (str): ë ˆìŠ¤í† ë‘ ID

  Returns:
      dict: ì²˜ë¦¬ ê²°ê³¼
  """
  try:
    # 1. ì„ íƒëœ ìƒìœ„ í‚¤ì›Œë“œë§Œ DBì— ì €ì¥ (ê¸°ì¡´ê³¼ ë™ì¼)
    keyword_id_map = {}
    for keyword in top_keywords:
      count = keyword_counts[keyword]
      keyword_id = importer._save_keyword(keyword, count, store_id)
      keyword_id_map[keyword] = keyword_id

    # 2. ì œí•œëœ ë¦¬ë·°-í‚¤ì›Œë“œ ê´€ê³„ë§Œ ì €ì¥ (ì†ŒìŠ¤ ì •ë³´ í¬í•¨)
    for review in limited_reviews:
      content = review.get("content", "")
      keyword = review.get("keyword", "")
      source = review.get("source", "naver")  # ì†ŒìŠ¤ ì •ë³´

      if keyword and keyword in keyword_id_map and content:
        importer._save_review_keyword_relation_with_source(content,
                                                           keyword_id_map[
                                                             keyword], source)

    # íŠ¸ëœì­ì…˜ ì»¤ë°‹
    importer.conn.commit()

    # ì†ŒìŠ¤ë³„ í†µê³„ ì •ë³´
    naver_total = sum(1 for r in limited_reviews if r.get("source") == "naver")
    blog_total = sum(1 for r in limited_reviews if r.get("source") == "blog")

    return {
      "success": True,
      "total_reviews": len(limited_reviews),
      "naver_reviews": naver_total,
      "blog_reviews": blog_total,
      "unique_keywords": len(top_keywords),
      "keyword_counts": {k: keyword_counts[k] for k in top_keywords}
    }

  except Exception as e:
    importer.conn.rollback()
    print(f"âŒ í‚¤ì›Œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return {
      "success": False,
      "error": str(e)
    }


# ë¸”ë¡œê·¸ ë¦¬ë·° ì²˜ë¦¬ í•¨ìˆ˜
def process_blog_reviews(restaurant_id):
  """
  ë¸”ë¡œê·¸ ë¦¬ë·° ì²˜ë¦¬ - author ë””ë ‰í† ë¦¬ êµ¬ì¡° ê³ ë ¤

  Args:
      restaurant_id (str): ë ˆìŠ¤í† ë‘ ID

  Returns:
      dict: ì²˜ë¦¬ ê²°ê³¼
  """
  # ë¸”ë¡œê·¸ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬
  blog_base_dir = f"/user/hadoop/big_final_rest_ugc_review_json/restaurant_id={restaurant_id}"

  # author ë””ë ‰í† ë¦¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
  author_dirs = get_blog_author_directories(blog_base_dir)

  if not author_dirs:
    print(f"âŒ ë¸”ë¡œê·¸ ë¦¬ë·° author ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {blog_base_dir}")
    return {"success": False, "error": "author ë””ë ‰í† ë¦¬ ì—†ìŒ", "content": ""}

  # ê° author ë””ë ‰í† ë¦¬ì—ì„œ ë¦¬ë·° ì¶”ì¶œ í›„ í•©ì¹˜ê¸°
  all_content = ""
  success = False

  print(f"ë¸”ë¡œê·¸ ë¦¬ë·° - ì´ {len(author_dirs)}ê°œ author ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì¤‘...")

  for author_dir in author_dirs:
    print(f"  - {author_dir} ì²˜ë¦¬ ì¤‘...")

    # í•´ë‹¹ author ë””ë ‰í† ë¦¬ ë¦¬ë·° ì²˜ë¦¬
    result = pipeline.process_directory(
        hdfs_directory=author_dir,
        system_prompt=system_prompt_blog
    )

    if result["success"]:
      all_content += result["content"] + "\n"
      success = True
    else:
      print(f"    âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

  return {
    "success": success,
    "content": all_content
  }


# ì „ì²´ ë ˆìŠ¤í† ë‘ ì²˜ë¦¬ ì‹œì‘
print(f"=== ì´ {len(restaurant_ids)}ê°œ ë ˆìŠ¤í† ë‘ì˜ ë¦¬ë·° ì²˜ë¦¬ ì‹œì‘ ===")
print(f"ì‹¤í–‰ ëª¨ë“œ: {'í‚¤ì›Œë“œ ì¶”ì¶œ' if extract_mode else 'DB ì €ì¥'}")
total_start_time = time.time()

for idx, restaurant_id in enumerate(restaurant_ids, 1):
  restaurant_id = str(restaurant_id)  # IDë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
  print(f"\n[{idx}/{len(restaurant_ids)}] ë ˆìŠ¤í† ë‘ ID {restaurant_id} ì²˜ë¦¬ ì‹œì‘")
  start_time = time.time()

  # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ (ì†ŒìŠ¤ë³„ë¡œ êµ¬ë¶„)
  naver_output_file = f"/keyword/restaurant_{restaurant_id}_naver_review_keywords.txt"
  blog_output_file = f"/keyword/restaurant_{restaurant_id}_blog_review_keywords.txt"
  combined_output_file = f"/keyword/restaurant_{restaurant_id}_all_review_keywords.txt"

  if extract_mode:
    # 1ë‹¨ê³„: HDFSì—ì„œ ë¦¬ë·° ë°ì´í„° ì¶”ì¶œí•˜ì—¬ íŒŒì¼ë¡œ ì €ì¥
    try:
      # ì¼ë°˜ ë¦¬ë·° ì²˜ë¦¬
      naver_review_dir = f"/user/hadoop/big_final_rest_review_json/restaurant_id={restaurant_id}"
      print(f"1-1ë‹¨ê³„: HDFSì—ì„œ ì¼ë°˜ ë¦¬ë·° ë°ì´í„° ê°€ì ¸ì™€ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
      naver_result = pipeline.process_directory(
          hdfs_directory=naver_review_dir,
          system_prompt=system_prompt_naver
      )

      # ë¸”ë¡œê·¸ ë¦¬ë·° ì²˜ë¦¬ - author ë””ë ‰í† ë¦¬ êµ¬ì¡° ê³ ë ¤
      print(f"1-2ë‹¨ê³„: HDFSì—ì„œ ë¸”ë¡œê·¸ ë¦¬ë·° ë°ì´í„° ê°€ì ¸ì™€ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
      blog_result = process_blog_reviews(restaurant_id)

      # ì¼ë°˜ ë¦¬ë·° ê²°ê³¼ ì €ì¥
      if naver_result["success"]:
        with open(naver_output_file, "w", encoding="utf-8") as f:
          f.write(naver_result["content"])
        print(f"âœ… ì¼ë°˜ ë¦¬ë·° ê²°ê³¼ê°€ {naver_output_file} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
      else:
        print(f"âŒ ì¼ë°˜ ë¦¬ë·° íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {naver_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

      # ë¸”ë¡œê·¸ ë¦¬ë·° ê²°ê³¼ ì €ì¥
      if blog_result["success"]:
        with open(blog_output_file, "w", encoding="utf-8") as f:
          f.write(blog_result["content"])
        print(f"âœ… ë¸”ë¡œê·¸ ë¦¬ë·° ê²°ê³¼ê°€ {blog_output_file} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
      else:
        print(f"âŒ ë¸”ë¡œê·¸ ë¦¬ë·° íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {blog_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

      # ë‘ ê²°ê³¼ í•©ì¹˜ê¸°
      if naver_result["success"] or blog_result["success"]:
        combined_content = ""
        if naver_result["success"]:
          combined_content += naver_result["content"] + "\n"
        if blog_result["success"]:
          combined_content += blog_result["content"]

        with open(combined_output_file, "w", encoding="utf-8") as f:
          f.write(combined_content)
        print(f"âœ… í†µí•© ê²°ê³¼ê°€ {combined_output_file} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
      print(f"âŒ ë ˆìŠ¤í† ë‘ ID {restaurant_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    # 2ë‹¨ê³„: ì €ì¥ëœ íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ DBì— ì €ì¥
  try:
    # í†µí•© íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(combined_output_file):
      print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {combined_output_file}")
      continue

    # íŒŒì¼ ë‚´ìš© ì½ê¸°
    with open(combined_output_file, "r", encoding="utf-8") as f:
      file_content = f.read()

    # JSON ê°ì²´ íŒŒì‹±
    all_reviews = parse_json_objects(file_content)

    if not all_reviews:
      print(f"âŒ íŒŒì‹±ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤")
      continue

    # ì†ŒìŠ¤ë³„ ë¦¬ë·° ì¹´ìš´íŠ¸
    naver_reviews = sum(1 for r in all_reviews if r.get("source") == "naver")
    blog_reviews = sum(1 for r in all_reviews if r.get("source") == "blog")

    print(
        f"ì´ {len(all_reviews)}ê°œ ë¦¬ë·° (ì¼ë°˜: {naver_reviews}ê°œ, ë¸”ë¡œê·¸: {blog_reviews}ê°œ)")

    # ë¦¬ë·° ì œí•œ (ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 3ê°œ, ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ)
    limited_reviews, keyword_counts, top_keywords = limit_reviews_by_keywords(
        all_reviews, MAX_KEYWORDS, MAX_REVIEWS)

    print(
        f"2ë‹¨ê³„: í‚¤ì›Œë“œ ë°ì´í„° DBì— ì €ì¥ ì¤‘... (ì „ì²´ {len(all_reviews)}ê°œ ì¤‘ {len(limited_reviews)}ê°œ ë¦¬ë·° ì„ íƒ)")

    # DB ì €ì¥
    db = PostgresImporter()

    try:
      # ì»¤ìŠ¤í…€ í•¨ìˆ˜ë¡œ ì œí•œëœ ë¦¬ë·°ì™€ ì „ì²´ ì¹´ìš´íŠ¸ ì €ì¥
      db_result = save_limited_reviews_with_source(
          db, limited_reviews, keyword_counts, top_keywords, restaurant_id)

      if db_result["success"]:
        print(
            f"âœ… DB ì €ì¥ ì™„ë£Œ: {db_result['total_reviews']} ë¦¬ë·° (ì¼ë°˜: {db_result['naver_reviews']}ê°œ, ë¸”ë¡œê·¸: {db_result['blog_reviews']}ê°œ), {db_result['unique_keywords']} ê³ ìœ  í‚¤ì›Œë“œ")

        # ìƒìœ„ í‚¤ì›Œë“œ ì¶œë ¥ (ì†ŒìŠ¤ë³„ ì¹´ìš´íŠ¸)
        print("\nğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ:")
        for keyword in top_keywords:
          count = keyword_counts[keyword]
          print(f"  - {keyword}: {count}íšŒ")
      else:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {db_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    finally:
      db.close()

  except Exception as e:
    print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

  # ì²˜ë¦¬ ì‹œê°„ ì¶œë ¥
  end_time = time.time()
  elapsed_time = end_time - start_time
  print(f"ë ˆìŠ¤í† ë‘ ID {restaurant_id} ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")

# ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ
total_elapsed_time = time.time() - total_start_time
print(f"\n=== ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ (ì´ ì†Œìš” ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ) ===")
